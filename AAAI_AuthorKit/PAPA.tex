%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
% AAAI format packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{comment}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem{rul}{Expansion Rule}
% END Additional packages
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (PARA*: Parallel Anytime Repairing A*)
/Author (Aram Ebtekar, Mike Phillips, Sven Koenig, Maxim Likhachev)
/Keywords (weighted A* search, parallel algorithm, heuristic)
}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{PARA*: Parallel Anytime Repairing A*}
\author{Aram Ebtekar$^\dagger$ \and Mike Phillips$^\dagger$ \and Sven Koenig\thanks{University of Southern California, Los Angeles, CA 90089} \and Maxim Likhachev% <-this % stops a space
\thanks{Carnegie Mellon University, Pittsburgh, PA 15217}% <-this % stops a space
%
}
\author{AAAI 2015 Submission X}% anonymizer
\maketitle
\begin{abstract}
\begin{quote}
Recent advances in processor performance increasingly come in the form of additional processors as opposed to faster ones. In order for heuristic searches to take advantage of these architectures, we must adapt the algorithms for parallel processing.
wPA*SE (weighted Parallel A* for Slow Expansions) is a recent parallel variant of A*, which guarantees a solution costing within a specified factor of optimal, while expanding each state at most once. wPA*SE can achieve a nearly linear speedup in the number of processor cores if expansions are sufficiently time-consuming to dominate the search runtime.
Much of the overhead of wPA*SE is due to the careful selection of the next state to expand, which is needed to maintain the theoretical properties.
In this work, we present Enhanced PA*SE (ePA*SE), which maintains speedups when expansions are faster than wPA*SE allows. ePA*SE reduces the overhead of wPA*SE in selecting states for expansion by maintaining tighter bounds on the suboptimality of each individual state.
We show comparable performance to wPA*SE when expansions are slow, and better performance as the number of cores increases and expansions become faster. On the theoretical side, ePA*SE provides the same guarantees on completeness and solution quality as wPA*SE. We also show how it generalizes single-source shortest paths, providing performance bounds in the massively parallel limit. Finally, we present PARA*, an anytime variant of ePA*SE.
\end{quote}
\end{abstract}

\section{Introduction}

Breadth-first and depth-first search are generalized by a class of frontier-based search algorithms, differing mainly in the means by which nodes are selected from the frontier for expansion. In the weighted A* algorithm, the choice combines a greedy goal-directed bias to reduce search time, with a breadth-first bias which guarantees suboptimality by a specified factor. With the advent of multi-core processors, making use of parallelism has become a priority for algorithm designers. Parallel A* for Slow Expansions (PA*SE) and its weighted variant wPA*SE offer nearly linear speedup in the number of cores, provided the search time is dominated by time-consuming expansions.

In this paper, we present Enhanced PA*SE (ePA*SE). Its performance at least rivals wPA*SE in general, and surpasses it when expansions times are faster or a lot of processor cores are available. These improvements are achieved by tightening the analysis of wPA*SE. This enables several theoretical results, which we also present.

Finally, we present  Parallel Anytime Repairing A* (PARA*), a simultaneous improvement over both Anytime Repairing A* (ARA*) and wPA*SE. ARA* and PARA* are anytime search algorithms, gradually reducing the goal-directed bias to improve solution cost as much as planning time allows.

Alternative selling point: our enhancements are two-fold. On one hand, we make algorithmic improvements which generalize the expansion rule, increasing parallelism while reducing the overhead for making use of said parallelism. On the other, we present a more detailed theoretical analysis, discussing the algorithm's properties in further detail.

\section{Problem Formulation}

We wish to find approximate single-pair shortest-paths. That is, given a directed graph with non-negative edge costs $c(s,s') \ge 0$, we must identify a path from $s_{start}$ to $s_{goal}$ whose cost is at most a specified factor $\epsilon\ge 1$ of the true distance $c^*(s_{start},s_{goal})$. Subject to this bounded optimality guarantee, we want to plan as quickly as possible.

We assume the distances can be estimated by a \textbf{consistent heuristic} $h$, meaning $h(s,s')\le c(s,s')$ and $h(s,s')\le h(s,s'') + h(s'',s')$ for all $s,s',s''$. Of course, consistency implies \textbf{admissibility}, meaning $h(s,s')\le c^*(s,s')$.

\section{Algorithm Design}

\subsection{A parallel view of wA*}

Many A* variants work by maintaining a set of estimates $g(s)$ bounding the optimal cost $g^*(s) = c^*(s_{start},s)$ of reaching $s$ from $s_{start}$. The estimates are constructive: every state $s$ in the search tree has a back-pointers $bp(s)$, and these can be followed back from $s$ to $s_{start}$ to yield a path of costing at most $g(s)$.

In hopes of avoiding duplicate effort, the A* variants we consider are designed to expand each node at most once. Thus, before expanding $s$, it's important to verify that we already have a near-optimal path from $s_{start}$ to $s$. Formally, we say a state $s$ is \textbf{safe for expansion} once we have deduced that $g(s) \le \epsilon g^*(s)$.

wA* sorts the frontier by the numeric keys $f(s) = g(s) + wh(s)$ where $w = \epsilon$. Let $bound(s)$ be defined by $g(s) + f(s') - f(s)$ where $s'$ is the first open state of the frontier, i.e. one with minimal $f$-value. Then
\begin{eqnarray*}
bound(s) &=& g(s) + \min_{s'}(f(s')) - f(s)
\\&=& \min_{s'}(g(s) + f(s') - f(s))
\\&=& \min_{s'}(g(s') + \epsilon(h(s',s_{goal}) - h(s,s_{goal}))) 
\\&\le& \min_{s'}(g(s') + \epsilon h(s',s))
\\&\le& \epsilon g^*(s')
\end{eqnarray*}

Therefore, a state $s$ can be considered safe for expansion if $g(s) \le bound(s)$, which of course reduces to $0 \le f(s') - f(s)$. In other words, $s$ must have the minimal $f$-value of the frontier. This can be stated as a principle:

\begin{rul}[wA* rule]
A state $s\in OPEN$ is safe for expansion if its $f$-value is minimal among states in the frontier.
\end{rul}

Already, this grants a trivial degree of parallelism: if multiple states have the same minimal $f$-value, they can be expanded simultaneously. The main contribution of wPA*SE is to generalize this principle, allowing more states to be simultaneously safe for expansion.

\subsection{Review of wPA*SE}

Before describing how wPA*SE generalizes the wA* rule for safe expansion, let's outline the entire search algorithm in more detail. Our presentation of wPA*SE differs slightly from the original version in \cite{asdf}, but the algorithm we describe is essentially equivalent. Algorithm \ref{alg:main} is a skeleton for wPA*SE. It begins by clearing the data structures and expanding out all edges coming from the start node.

Intuitively, $OPEN$ represents the frontier of states which are candidates for expansion, initially consisting of the direct successors of $s_{start}$. Once a safe state is identified and selected for expansion, it's removed from $OPEN$ and inserted into $BE$ and $CLOSED$. $BE$ represents the freshly $CLOSED$ states; they are still in the process of being expanded, but are about to leave the frontier. Its cardinality $|BE|$ will never exceed the number of threads.

\begin{algorithm}
\caption{main()}
\label{alg:main}
\begin{algorithmic}
\STATE $OPEN := BE := CLOSED := FROZEN := \emptyset$
\STATE $g(s_{start}) := 0$
\STATE expand($s_{start}$)
\STATE run search() on multiple threads in parallel
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{search()}
\label{alg:search}
\begin{algorithmic}
\WHILE{$g(s_{goal}) > bound(s_{goal})$}
\STATE among $s\in OPEN$ such that $g(s) \le bound(s)$, remove one with the smallest $f(s)$ and LOCK $s$
\IF{such an $s$ does not exist}
\STATE wait until $OPEN$ or $BE$ change
\STATE continue
\ENDIF
\STATE insert $s$ into $CLOSED$
\STATE insert $s$ into $BE$ with key $f(s)$
\STATE $v_{expand} := g(s)$
\STATE UNLOCK $s$
\STATE expand($s$)
\STATE $v(s) := v_{expand}$
\STATE remove $s$ from $BE$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{expand($s$)}
\label{alg:expand}
\begin{algorithmic}
\FORALL{$s' \in successors(s)$}
\STATE LOCK $s'$
\IF{$s'$ has not been generated yet}
\STATE $g(s') := v(s') := \infty$
\ENDIF
\IF {$g(s') > g(s) + c(s,s')$}
\STATE $g(s') = g(s) + c(s,s')$
\STATE $bp(s') = s$
\IF{$s' \in CLOSED$}
\STATE insert $s'$ in $FROZEN$
\ELSE
\STATE insert/update $s'$ in $OPEN$ with key $f(s')$
\ENDIF
\ENDIF
\STATE UNLOCK $s'$
\ENDFOR
\end{algorithmic}
\end{algorithm}

Every thread of wPA*SE runs Algorithm \ref{alg:search} in parallel. $OPEN$ and $BE$ are represented by balanced binary trees sorted by the key values $f(s) = g(s) + wh(s,s_{goal})$ for some parameter $w\ge 0$. Usually we recommend setting $w=\epsilon$, but possible motivations to select alternatives are discussed later.

Each thread begins by attempting to identify and extract an element $s\in OPEN$ which is safe for expansion. Each time a thread finds a safe $s$, it performs an expansion as described in Algorithm \ref{alg:expand}. The search terminates once the goal is safe for expansion.

The assignable variables $v(s)$ and the $FROZEN$ list are never used, and exist in the pseudocode only to aid the analysis. Intuitively, $v(s)$ is the distance label held by $s$ during its most recent expansion. If $g(s) < v(s)$, $s$ should be a candidate for future expansion. $FROZEN$ consists of $CLOSED$ nodes for which $g(s) < v(s)$, and hence would be candidates for expansion if not for the fact that $s$ was already expanded. Thus, $OPEN\cup FROZEN$ is precisely the set of states $s$ for which $g(s) < v(s)$. All other states have $g(s) = v(s)$.

There are many ways to implement the auxiliary function $bound(s)$; as discussed in the previous section, we obtain a trivially parallelized version of wA* by using $bound(s) = g(s) + f(s') - f(s)$ for the minimizing $s'\in OPEN \cup BE$. Provided our implementation satisfies $bound(s) \le \epsilon g^*(s)$, the same argument applies to show that the condition $g(s) \le bound(s)$ suffices to ensure $s$ is safe for expansion.

\begin{rul}[wPA*SE rule]
A state $s\in OPEN$ is safe for expansion if its $g(s) \le bound(s)$ using the implementation of $bound$ listed in Algorithm \ref{alg:aux}.
\end{rul}

To sketch the intuition behind the wPA*SE implementation of $bound$, we argue that in order for $s\in OPEN$ to be unsafe for expansion, there must be an optimal path from $s$ passing through some $s'\in OPEN\cup BE$. Thus,
\[g^*(s) = g^*(s') + c^*(s',s) \ge g^*(s') + h(s',s).\]
It can be shown that, if some $s'$ makes $s$ unsafe, then there is such an $s'$ whose $g$-value is $\epsilon$-optimal. For $s$ to be safe, it then suffices that $g(s) \le g(s') + \epsilon h(s', s)$ since the latter quantity is at most $\epsilon (g^*(s') + h(s',s)) \le \epsilon g^*(s)$.

Assuming $w \le \epsilon$, this inequality is guaranteed to hold whenever $f(s') \ge f(s)$. Hence, it suffices to check $s'$ for which $f(s') < f(s)$. See \cite{} for a proof that wPA*SE is $\max(w, \epsilon)$-optimal. In fact, it can be made $\epsilon$-optimal, but when $w > \epsilon$ that requires checking  all $s'\in OPEN\cup BE$ instead of just those with smaller $f$-value than $s$. Even in the former case, these checks are expensive, their cost per state being at least proportional to the parallelism. The principal aim of our enhancements is to substantially reduce the number of checks needed while increasing parallelism.

Before continuing, we briefly note that atomic locks are used for concurrency. For conceptual clarity, the mechanism presented here is considerably simpler than our C++ implementation. We will not discuss details here, but it bears mentioning that every use of the main data structures is guarded by the same global lock.

\begin{algorithm}
\caption{Auxiliary Functions}
\label{alg:aux}
\begin{algorithmic}
\STATE \textbf{FUNCTION} $successors(s)$
\RETURN $\{s' \mid c(s,s')<\infty\}$

\STATE \textbf{FUNCTION} $f(s)$
\RETURN $g(s) + wh(s,s_{goal})$

\STATE \textbf{FUNCTION} $bound(s)$
\STATE $g_{front} := g(s)$
\STATE $s' :=$ first node in $OPEN \cup BE$
\WHILE{$f(s') < f(s)$ \AND $g(s) \le g_{front}$}
\STATE $g_{front} := \min(g_{front},\;g(s') + \epsilon h(s',s))$
\STATE $s' :=$ node following $s'$ in $OPEN \cup BE$
\ENDWHILE
\RETURN $g_{front}$
\end{algorithmic}
\end{algorithm}

\subsection{ePA*SE}

Having conveniently rephrased wPA*SE for our purposes, we are now prepared to enhance the algorithm.

We introduce the assignable variables $g_p(s)$. Their semantics are similar to $bound(s)$ but somewhat more intricate. While $bound(s)/\epsilon$ is a lower bound on the unrestricted distance $g^*(s)$, $g_p(s)/\epsilon$ is a lower bound on the cost from $s_{start}$ to $s$, restricting ourselves to paths in which $s$ is immediately preceded by an expanded node. That is, $g_p(s) \le \epsilon (c^*(s_{start},s') + c(s',s))$ for all $s'\in CLOSED$. To maintain this invariant, we initialize $g_p(s)$ to $\infty$ just as Algorithm \ref{alg:expand} did for $g(s)$ and $v(s)$, and then add the following line immediately before the second \textbf{if} statement in expand($s$):
\[g_p(s') := \min(g_p(s'),\; b + \epsilon c(s,s'))\]

Here, $b$ is the lower bound on $\epsilon g^*(s)$ computed by the $bound(s)$ call when $s$ was extracted, or $0$ if $s=s_{start}$. Note that at all times, $g(s) < \infty \Rightarrow g(s) < g_p(s)$.

The performance gains of ePA*SE come from changing the implementation of $bound(s)$ to the version shown in Algorithm \ref{alg:eaux}. It now makes use of a constant $c_l \ge 0$, denoting the best known lower bound on the graph's edge costs. $c_l$ can be 0 if we are agnostic about the possible costs, but ePA*SE can make use of larger bounds if available.

\begin{rul}[ePA*SE rule]
A state $s\in OPEN$ is safe for expansion if its $g(s) \le bound(s)$ using the implementation of $bound$ listed in Algorithm \ref{alg:eaux}.
\end{rul}

Recall that wPA*SE upper-bounds $\epsilon g^*(s)$ by the minimum of $g(s') + \epsilon h(s',s)$ for $s'\in OPEN\cup BE$. Hoever, not every element of $OPEN\cup BE$ needs to be considered separately. Indeed, suppose we choose an arbitrary subset $V\subseteq OPEN\cup BE$ over which we explicitly minimize $g(s') + \epsilon h(s',s)$. As we saw when discussing wA*, $g(s) + f(s') - f(s) \le g(s') + \epsilon h(s',s)$. Thus, if we let $\bar V = (OPEN\cup BE)\setminus V$, a valid implementation of $bound(s)$ produces 
\[\min\left(\min_{s'\in V}\left(g(s') + \epsilon h(s',s)\right),\;g(s) + \min_{s'\in\bar V} f(s') - f(s)  \right).\]

wA* can be seen as the instance of this general definition with $V = \emptyset$. Since $OPEN$ and $BE$ are sorted, a minimizing element of $\bar V$ is easily found. In PA*SE, $V$ consists of the states $s'$ such that $f(s') < f(s)$. The term corresponding to $\bar V$ is trivially minimized by $s$, yielding the value $g(s)$. Nonetheless, the term corresponding to $V$ can take substantial effort to compute. Much of this effort can be removed by decreasing the size of $V$.

Before doing so, we note a few optimizations which will be justified in the formal analysis. Firstly, we can replace $g$ by $g_p$ in the term $g(s') + \epsilon h(s',s)$. Since $g_p(s) > g(s)$, this can only make the condition $g(s) \le bound(s)$ more likely to hold, thus increasing parallelism. Likewise, if $c_l > 0$, the $\bar V$ term can be improved using Lemma \ref{lem:indep}.

It remains only to define $V$. Larger $V$ tightens (i.e. increases) the value of $bound(s)$, but increases the computational expense. In ePA*SE, we begin with $V = \emptyset$ and iteratively add elements from $OPEN\cup BE$ beginning with the smallest $f$-values. We continue this until we have determined with certainty whether or not $\min_{s'\in V}\left(g_p(s') + \epsilon h(s',s)\right)$ (i.e. the value we would obtain if we let $V = OPEN\cup BE$) is greater than $g(s)$. That is, we do the minimum possible work while ensuring the result of the check $g(s) \le bound(s)$ matches what would be obtained in the case where $V$ is the whole frontier.

$g_{front}$ is the bound computed from $V$, while $g_{back}$ is computed from $\bar V$. The former decreases and the latter increases monotonically as states $s'$ are added to $V$. The first termination condition $g_{back} \ge g(s)$ corresponds to a point after which the result of the comparison $g(s) \le bound(s)$ cannot change by growing $V$, because $g_{back} \ge g(s)$ would continue to hold thereafter and Lemma \ref{lem:indep} forbids any future elements moving from $\bar V$ to $V$ from changing the result. This is guaranteed to hold for $s' = s$, ensuring that $V$ never takes elements which would not have been considered by wPA*SE. Similarly, the second termination condition $g(s) > g_{front}$ corresponds to a guarantee that $g(s) \le bound(s)$ can never hold no matter how $V$ is defined.

In summary, the ePA*SE implementation yeilds a sharper comparison than wPA*SE while doing explicit checks against no more, and often many fewer, states of the frontier. For instance, all states whose $f$-values are within $(2\epsilon-w-1)c_l$ of the minimum can be expanded in parallel, as can any set of states which wPA*SE considers safe for expansion.

TODO: Prove completeness if finite out-degree, $w<\infty$ and $c_l > 0$

\subsection{PARA*: Parallel Anytime Reparing A*}

Finally, we note that by analogy with ARA*, ePA*SE can be made into an anytime algorithm, iteratively computing solutions with progressively smaller suboptimality bounds. We can modify Algorithm \ref{alg:main} so that, instead of calling the parallel search() only once, search() is called in a loop which terminates only when the agent decides it's no longer worthwhile to spend additional planning time to improve the solution. Between iterations, the thaw() procedure in Algorithm \ref{alg:eaux} must be called to place the $FROZEN$ states back into the $OPEN$ list, since any state with $g(s) < v(s)$ can potentially be expanded to improve other $g$-values.

The inequality $g_p(s) \le \epsilon (c^*(s_{start},s') + c(s',s))$, which defines $g_p$, must hold for every $s'$ which was ever expanded (hence $CLOSED$), including during prior anytime iterations. Hence, the $g_p$ values must be reset when $\epsilon$ decreases. thaw() already does this for the $OPEN$ list. When another state is seen for the first time in the present iteration (or equivalently, expand() encounters an $s'\notin OPEN\cup CLOSED$), it performs the reset operation
\[g_p(s') := g(s') + 2(\epsilon-1)c_l\]

This is a generalization of the $g_p(s') := \infty$ step from the non-anytime version.

\begin{algorithm}
\caption{Auxiliary Functions ePA*SE}
\label{alg:eaux}
\begin{algorithmic}
\STATE \textbf{FUNCTION} $g_{back}(s',s)$
\IF{$s' = NULL$}
\RETURN $\infty$
\ELSIF{$w \le \epsilon$}
\RETURN $g(s) + f(s') - f(s) + (2\epsilon-w-1)c_l$
\ELSE
\RETURN $\frac\epsilon w\left(g(s) + f(s') - f(s)\right) + (\epsilon-1)c_l$
\ENDIF

\STATE \textbf{FUNCTION} $bound(s)$
\STATE $g_{front} := g_p(s)$
\STATE $s' :=$ first node in $OPEN \cup BE$
\WHILE{$g_{back}(s',s) < g(s) \le g_{front}$}
\STATE $g_{front} := \min(g_{front},\;g_p(s') + \epsilon h(s',s))$
\STATE $s' :=$ node following $s'$ in $OPEN \cup BE$
\ENDWHILE
\RETURN $\min(g_{front},\;g_{back}(s',s))$

\STATE \textbf{PROCEDURE} $thaw()$
\STATE choose new $\epsilon \in [1,\infty]$ and $w \in [0,\infty]$
\STATE $OPEN := OPEN \cup FROZEN$ with keys $f(s)$
\STATE $CLOSED := FROZEN := \emptyset$
\FORALL{$s\in OPEN$}
\STATE $g_p(s) := g(s) + (\epsilon-1)\min(g(s),\;2c_l)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

The following lemma lists some easily checked invariants of wPA*SE, ePA*SE and PARA*.

\begin{lemma}
\label{lem:prop}
At all times, the following invariants hold:
\begin{itemize}
\item $OPEN\cap CLOSED = \emptyset$
\item $BE\cup FROZEN \subseteq CLOSED$
\item $s\in OPEN\cup BE\cup FROZEN \Leftrightarrow g(s) < v(s)$
\item $s\notin OPEN\cup BE\cup FROZEN \Leftrightarrow g(s) = v(s)$
\item $g(bp(s)) + c(bp(s),s) \le g(s) \le min_{s'}\{v(s') + c(s',s)\}$
\item Following $bp(\cdot)$ from $s$ yields a path costing at most $g(s)$
\item $s\in OPEN\cup CLOSED \Rightarrow g(s) + (\epsilon-1)c_l \le g_p(s) \le \epsilon g(s)$
\item $s\in OPEN\cup CLOSED$ iff we had $g(s)<v(s)$ sometime during the current main() loop iteration
\end{itemize}
\end{lemma}

\begin{proof}
Induction on time.
\end{proof}

\section{Analysis}

In addition to making algorithmic enhancements, we present a deeper analysis than the wPA*SE paper \cite{}. We begin by looking at the properties of $bound$ from Algorithm \ref{alg:eaux}.

\begin{lemma}
\label{lem:indep}
At all times, for all states $s$ and $s'\notin \{s_{start},s\}$:
\[g_{back}(s',s) \le g_p(s') + \epsilon h(s',s).\]
\end{lemma}

\begin{proof}
If $w \le \epsilon$, then
\begin{eqnarray*}
&&g(s) + f(s') - f(s) + (2\epsilon-w-1)c_l
\\&=& g(s') + w(h(s',s_{goal}) - h(s,s_{goal})) + (2\epsilon-w-1)c_l
\\&\le& g(s') + wh(s',s) + (2\epsilon-w-1)c_l
\\&\le& g(s') + \epsilon h(s',s) + (w-\epsilon)c_l + (2\epsilon-w-1)c_l
\\&=& g(s') + (\epsilon-1)c_l + \epsilon h(s',s)
\\&\le& g_p(s') + \epsilon h(s',s)
\end{eqnarray*}
On the other hand, if $w > \epsilon$, then
\begin{eqnarray*}
&&\frac\epsilon w\left(g(s) + f(s') - f(s)\right) + (\epsilon-1)c_l
\\&=& \frac\epsilon w\left(g(s') + w(h(s',s_{goal}) - h(s,s_{goal})) \right) + (\epsilon-1)c_l
\\&\le& g(s') + \epsilon(h(s',s_{goal}) - h(s,s_{goal})) + (\epsilon-1)c_l
\\&\le& g(s') + (\epsilon-1)c_l + \epsilon h(s',s)
\\&\le& g_p(s') + \epsilon h(s',s)
\end{eqnarray*}
\end{proof}

\begin{lemma}
\label{lem:bound}
For all $s\in OPEN\cup BE$, $bound(s) \le \min_{s'\in OPEN \cup BE} g_p(s') + \epsilon h(s',s)$. Furthermore, $g(s) \le bound(s)$ iff $g(s) \le \min_{s'\in OPEN \cup BE} g_p(s') + \epsilon h(s',s)$.
\end{lemma}

\begin{proof}
By construction, $bound(s)$ is bounded above by $g_p(s') + \epsilon h(s',s)$ for $s'=s$ as well as for the other states $s'$ which are checked in the loop. As for the remaining states $s' \in OPEN \cup BE$, the algorithm ensures that $bound(s) \le g_{back}(s',s)$ for these by using a minimum representative. By Lemma \ref{lem:indep}, it follows that
\[bound(s) \le \min_{s' \in OPEN \cup BE} g_p(s') + \epsilon h(s',s).\]

To prove the second claim, note that the loop in $bound(s)$ terminates under only two conditions. Either $g(s) > g_{front}$, in which case we have $g(s) > g_p(s') + \epsilon h(s',s) \ge bound(s)$ for the $s'$ which began the final iteration; or $g(s) \le g_{back}(s',s)$, in which case $g(s) \le bound(s)$ iff $g(s) \le g_{front}$ iff $g(s) \le g_p(s') + \epsilon h(s',s)$ for all $s' \in OPEN \cup BE$.
\end{proof}

\begin{thm}
\label{thm:subopt}
For all $s\in OPEN\cup BE$, $bound(s) \le \epsilon g^*(s)$. Hence, for all $s\in CLOSED$, $g(s) \le v(s) \le \epsilon g^*(s)$.
\end{thm}

\begin{proof}
We proceed by induction on the order in which states are expanded.

Let $\pi = \langle s_0,s_1,\ldots,s_N \rangle$ be a minimum-cost path from $s_0 = s_{start}$ to $s_N = s\in OPEN\cup BE$. Choose the minimum $i$ such that $s_i\in OPEN\cup BE$. If $i = 1$, then
\[g_p(s_i) \le \epsilon g(s_i) = \epsilon g^*(s_i)\]
If $i \ge 2$, there are two cases to consider, depending on whether $s_{i-1}\in CLOSED$.

If so, then $expand(s_{i-1})$ has assigned to $g_p(s_i)$. Hence by the induction hypothesis,
\begin{eqnarray*}
g_p(s_i) &\le& v(s_{i-1}) + \epsilon c(s_{i-1},s_i)
\\&\le& \epsilon g^*(s_{i-1}) + \epsilon c(s_{i-1},s_i)
\\&=& \epsilon g^*(s_i)
\end{eqnarray*}

On the other hand, suppose $s_{i-1}\notin CLOSED$. Choose the maximum $j<i$ such that $s_j\in CLOSED$, or $j=0$ if there is no such $j$. Then $j\le i-2$ and, by the induction hypothesis, $g(s_j)\le \epsilon g^*(s_j)$. Furthermore, $g(s_k) = v(s_k)$ for all $j<k<i$. Let $g_{old}(s_i)$ denote the value of $g(s_i)$ at the start of the current main() loop iteration. Then,
\begin{eqnarray*}
g_p(s_i) &\le& g_{old}(s_i) + 2(\epsilon-1)c_l
\\&\le& v(s_j) + c^*(s_j,s_i) + 2(\epsilon-1)c_l
\\&\le& \epsilon g^*(s_j) + c^*(s_j,s_i) + 2(\epsilon-1)c_l
\\&=& \epsilon (g^*(s_j) + c^*(s_j,s_i)) + (\epsilon-1)(2c_l - c^*(s_j,s_i))
\\&\le& \epsilon g^*(s_i)
\end{eqnarray*}

In all three cases, we found that
\[g_p(s_i) + \epsilon h(s_i,s) \le \epsilon g^*(s_i) + \epsilon c^*(s_i,s) = \epsilon g^*(s).\]
Therefore, by Lemma \ref{lem:bound},
\[bound(s) \le \min_{s' \in OPEN \cup BE} g_p(s') + \epsilon h(s',s) \le \epsilon g^*(s).\]
\end{proof}

\begin{cor}
\label{cor:subopt}
At the end of a main() loop iteration, the path obtained by following the back-pointers $bp(\cdot)$ from $s_{goal}$ to $s_{start}$ is $\epsilon$-suboptimal.
\end{cor}

\begin{proof}
The termination condition of PARA* implies $g(s_{goal}) \le bound(s_{goal})$. By construction, the path given by following back-pointers costs at most $g(s_{goal})$. The claim now follows from Theorem \ref{thm:subopt}.
\end{proof}

\section{Performance Guarantees - Blind PARA*}

By deleting the while loop in bound($s$), we arrive at a simplified version of the algorithm which we call Blind PARA*. $g_p$ values are no longer used, so their computation can be ommitted. Blind PARA* can only expand states which would be proved safe in PARA* using zero iterations of the bound($s$) loop. Thus, every performance guarantees that we prove for Blind PARA* also holds for PARA*. 

\begin{thm}
\label{thm:depth}
If $w \le 1$, the parallel depth of Blind PARA* is bounded above by
\[\min\left(\frac{\epsilon g^*(s_{goal})}{(1-w)c_l},\;
\frac{\left(\epsilon g^*(s_{goal})\right)^2 }{(4\epsilon-2w-2)c_l^2}\right).\]
\end{thm}

\begin{proof}
We prove the two bounds separately. For the first, note that if the lowest $f$-value is $f_{min}$, every state with $f$-value up to $f_{min} + (2\epsilon-w-1)c_l$ can simultaneously be expanded. Since $h$ is consistent, the successors' $f$-values is at least $f_{min} + (1-w)c_l$. Therefore, the depth is at most
\[\frac{\epsilon g^*(s_{goal})}{(1-w)c_l}\]

For the other bound, write $t$ for $2\epsilon-w-1$. Notice that since $f$-values never decrease along paths, once the minimum $f$-value in $OPEN$ surpasses $f_{min}$, from then on all nodes with $f$-value up to $f_{min} + tc_l$ are always safe for expansion. And during each iteration of the simultaneous expansions, the $g$-value of all such nodes increases by at least $c_l$. Since $g$ cannot exceed $f$, this continues for at most $(f_{min} + tc_l) / c_l = f_{min}/c_l + t$ iterations, after which every node in $OPEN$ has $f$-value $\ge f_{min} + tc_l$. Continuing this process until $f_{min}$ exceeds $\epsilon g^*(s_{goal})$, a bound on the total iteration count is (TODO: fix this analysis)

\begin{eqnarray*}
&&t + 2t + 3t + ... + \epsilon g^*(s_{goal})/c_l
\\&\le& \epsilon  g^*(s_{goal})/(tc_l)(2+\epsilon g^*(s_{goal})/c_l+t)/2
\\&\le& (\epsilon g^*(s_{goal})/c_l )^2 (tc_l/(\epsilon g^*(s_{goal})) + 1/(2t) + c_l/(2\epsilon g^*(s_{goal})))
\\&\le& (\epsilon g^*(s_{goal})/c_l )^2 (t + 1/(2t) + 1/2)
\end{eqnarray*}
\end{proof}

\section{Edgewise Supobtimality}

Let $k(s)$ be the least number of edges used in a minimum-cost path to $s$ and fix $\delta > 0$. If $g_{front}$ and $g_{back}$ are each increased by $2\delta$, then by similar arguments to the proofs earlier in the paper, we find that, upon expanding $s$, $g(s) \le \epsilon g^*(s) + \delta k(s)$.

Here's an extension inspired by \cite{klein1997randomized}: suppose the mean edge cost $c_m$ along the optimal path is known to be much greater than the lower bound $c_l$. In such a case, the bound in Theorem \ref{thm:depth} scales poorly. To remedy the situation, we ``grow" the small edges, effectively running PARA* with $c_l' = c_l + \delta$ and $c'(s,s') = \max(c(s,s'), c_l')$.

\begin{thm}
\label{thm:delta}
If the mean cost of the edges along the minimum-cost path to $s$ is at least $c_m$, then upon expansion, $g(s) \le \epsilon(1+\delta/c_m)g^*(s)$. Therefore, to get the same optimality factor as $\epsilon$, we can set $\delta = (\epsilon-1)c_m$.
\end{thm}

\begin{proof}
We assumed $c_m \le g^*(s) / k(s)$, so $k(s) \le g^*(s) / c_m$.
It follows from Lemma \ref{thm:subopt} that $g'(s) \le \epsilon g'^*(s) \le \epsilon(g^*(s) + \delta k(s)) \le \epsilon(1+\delta/c_m)g^*(s)$.
\end{proof}

\begin{cor}
\label{cor:delta}
If $w \le 1$ and $c_m \le g^*(s) / k(s)$, the parallel depth of Blind PARA* can be improved to
\[\frac{\epsilon g^*(s_{goal})}{(1-w)(c_l+(\epsilon-1)c_m)}.\]
If in addition $c_m \ge g^*(s)/(mk(s))$, the depth is at most
\[\frac{\epsilon mk(s)}{(1-w)(\epsilon-1)}\]
\end{cor}

In other words, if we know the mean edge cost up to a small constant factor, we can find approximately optimal paths in a depth which is within a small factor of the ``omnicient" algorithm that expands only along the optimal path.

\section{Experiments}

\section{Conclusion}

\bibliographystyle{aaai}
\bibliography{PARA}

\end{document}
