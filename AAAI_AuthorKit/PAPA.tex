%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
% AAAI format packages
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
% Additional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{comment}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
% END Additional packages
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (PARA*: Parallel Anytime Repairing A*)
/Author (Aram Ebtekar, Mike Phillips, Sven Koenig, Maxim Likhachev)
/Keywords (weighted A* search, parallel algorithm, heuristic)
}
\setcounter{secnumdepth}{0}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{PARA*: Parallel Anytime Repairing A*}
\author{Aram Ebtekar$^\dagger$ \and Mike Phillips$^\dagger$ \and Sven Koenig\thanks{University of Southern California, Los Angeles, CA 90089} \and Maxim Likhachev% <-this % stops a space
\thanks{Carnegie Mellon University, Pittsburgh, PA 15217}% <-this % stops a space
%
}
\author{AAAI 2015 Submission X}% anonymizer
\maketitle
\begin{abstract}
\begin{quote}
PARA* is an anytime parallel heuristic search algorithm based on ARA* and PA*SE, which are in turn based on A*.
\end{quote}
\end{abstract}

\section{Fancy Stuff}

\begin{algorithm}
\caption{Auxiliary Functions}
\label{alg:aux}
\begin{algorithmic}
\STATE \textbf{FUNCTION} $f(s)$
\RETURN $g(s) + wh(s,s_{goal})$
\STATE \textbf{FUNCTION} $g_{back}(s',s)$
\IF{$w \le \epsilon$}
\RETURN $g(s) + f(s') - f(s) + (2\epsilon-w-1)c_l$
\ELSE
\RETURN $\frac\epsilon w\left(g(s) + f(s') - f(s)\right) + (\epsilon-1)c_l$
\ENDIF
\STATE \textbf{FUNCTION} $bound(s)$
\STATE $g_{front} := \infty$
\STATE $s' :=$ first node in $OPEN \cup BE$
\WHILE{$g_{back}(s',s) < g(s) \le g_{front}$}
\STATE $g_{front} := \min(g_{front},\;g_p(s') + \epsilon h(s',s))$
\STATE $s' :=$ node following $s'$ in $OPEN \cup BE$
\ENDWHILE
\RETURN $\min(g_{front},\;g_{back}(s',s))$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{expand($s$)}
\label{alg:expand}
\begin{algorithmic}
\FORALL{$s' \in successors(s)$}
\STATE LOCK $s'$
\IF{$s'$ was not yet seen in this main() iteration}
\IF{$s'$ has not been generated yet}
\STATE $g(s') := v(s') := \infty$
\ENDIF
\STATE $g_p(s') := g(s') + 2(\epsilon-1)c_l$
\ENDIF
\STATE $g_p(s') = \min(g_p(s'),\; g_{bound} + \epsilon c(s,s'))$
\IF {$g(s') > g(s) + c(s,s')$}
\STATE $g(s') = g(s) + c(s,s')$
\STATE $bp(s') = s$
\IF{$s' \in CLOSED$}
\STATE insert $s'$ in $FROZEN$
\ELSE
\STATE insert/update $s'$ in $OPEN$ with key $f(s')$
\ENDIF
\ENDIF
\STATE UNLOCK $s'$
\ENDFOR

\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{PARA*}
\label{alg:PARA*}
\begin{algorithmic}
\WHILE{$g(s_{goal}) > bound(s_{goal})$}
\STATE among $s\in OPEN$ such that $g(s) \le bound(s)$, remove one with the smallest $f(s)$ and LOCK $s$
\IF{such an $s$ does not exist}
\STATE wait until $OPEN$ or $BE$ change
\STATE continue
\ENDIF
\STATE $g_{bound} := bound(s)$
\STATE $v_{expand} := g(s)$
\STATE insert $s$ into $CLOSED$
\STATE insert $s$ into $BE$ with key $f(s)$
\STATE UNLOCK $s$
\STATE expand($s$)
\STATE $v(s) := v_{expand}$
\STATE remove $s$ from $BE$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{main()}
\label{alg:main}
\begin{algorithmic}
\STATE $OPEN := BE := \emptyset$
\STATE $FROZEN := \{s_{start}\}$
\STATE $g(s_{start}) := 0$
\REPEAT
\STATE choose $\epsilon \in [1,\infty]$ and $w \in [0,\infty]$
\STATE $OPEN := OPEN \cup FROZEN$ with keys $f(s)$
\STATE $CLOSED := FROZEN := \emptyset$
\FORALL{$s\in OPEN$}
\STATE $g_p(s) := g(s) + 2(\epsilon-1)c_l$
\ENDFOR
\STATE run PARA* on multiple threads in parallel
\UNTIL{path is good enough or planning time runs out}
\end{algorithmic}
\end{algorithm}

We assume all edge costs are bounded below by $c_l$. $h$ must be consistent: $h(s,s') \le c(s,s')$ and $h(s,s') \le h(s,s'')+h(s'',s')$ for all $s,s',s''$. For most applications, we recommend using $w = \epsilon$. However, our analysis will show that using small $w$ yields strong parallelism guarantees. All operations on the data structures $OPEN,BE,CLOSED,FROZEN$ are assumed to be atomic, i.e. they are implicitly preceded and succeeded by synchronous locks and unlocks to the data structure, respectively. $v(s)$ is included to aid the analysis but is never used in the algorithm.

\begin{lemma}
\label{lem:prop}
At all times, the following invariants hold:
\begin{itemize}
\item $OPEN\cap CLOSED = \emptyset$
\item $BE\cup FROZEN \subset CLOSED$
\item $s\in OPEN\cup BE\cup FROZEN \Rightarrow g(s) < v(s)$
\item $s\notin OPEN\cup BE\cup FROZEN \Rightarrow g(s) = v(s)$
\item $g(bp(s)) + c(bp(s),s) \le g(s) \le min_{s'}\{v(s') + c(s',s)\}$
\item Following $bp(\cdot)$ from $s$ yields a path costing at most $g(s)$
\item $g(s) + (\epsilon-1)c_l \le g_p(s)$
\item $s\in OPEN\cup CLOSED$ iff we had $g(s)<v(s)$ sometime during the current main() loop iteration
\end{itemize}
\end{lemma}

\begin{proof}
Induction on time.
\end{proof}

\begin{lemma}
\label{lem:indep}
At all times, for all states $s,s'$:
\[g_{back}(s',s) \le g_p(s') + \epsilon h(s',s).\]
\end{lemma}

\begin{proof}
If $w \le \epsilon$, then
\begin{eqnarray*}
&&g(s) + f(s') - f(s) + (2\epsilon-w-1)c_l
\\&=& g(s') + w(h(s',s_{goal}) - h(s,s_{goal})) + (2\epsilon-w-1)c_l
\\&\le& g(s') + wh(s',s) + (2\epsilon-w-1)c_l
\\&\le& g(s') + \epsilon h(s',s) + (w-\epsilon)c_l + (2\epsilon-w-1)c_l
\\&=& g(s') + (\epsilon-1)c_l + \epsilon h(s',s)
\\&\le& g_p(s') + \epsilon h(s',s)
\end{eqnarray*}
On the other hand, if $w > \epsilon$, then
\begin{eqnarray*}
&&\frac\epsilon w\left(g(s) + f(s') - f(s)\right) + (\epsilon-1)c_l
\\&=& \frac\epsilon w\left(g(s') + w(h(s',s_{goal}) - h(s,s_{goal})) \right) + (\epsilon-1)c_l
\\&\le& g(s') + \epsilon(h(s',s_{goal}) - h(s,s_{goal})) + (\epsilon-1)c_l
\\&\le& g(s') + (\epsilon-1)c_l + \epsilon h(s',s)
\\&\le& g_p(s') + \epsilon h(s',s)
\end{eqnarray*}
\end{proof}

\begin{lemma}
\label{lem:bound}
For all states $s$, $bound(s) \le \min_{s'\in OPEN \cup BE} g_p(s') + \epsilon h(s',s)$. Furthermore, $g(s) \le bound(s)$ iff $g(s) \le \min_{s'\in OPEN \cup BE} g_p(s') + \epsilon h(s',s)$.
\end{lemma}

\begin{proof}
By construction, $bound(s)$ is bounded above by $g_p(s') + \epsilon h(s',s)$ for states $s'$ which are checked in the loop. As for the remaining states $s' \in OPEN \cup BE$, the algorithm ensures that $bound(s) \le g_{back}(s',s)$ for these by using a minimum representative. By Lemma \ref{lem:indep}, it follows that
\[bound(s) \le \min_{s' \in OPEN \cup BE} g_p(s') + \epsilon h(s',s).\]

To prove the second claim, note that the loop in $bound(s)$ terminates under only two conditions. Either $g(s) > g_{front}$, in which case we have $g(s) > g_p(s') + \epsilon h(s',s) \ge bound(s)$ for the $s'$ which began the final iteration; or $g(s) \le g_{back}$, in which case $g(s) \le bound(s)$ iff $g(s) \le g_{front}$ iff $g(s) \le g_p(s') + \epsilon h(s',s)$ for all $s' \in OPEN \cup BE$.
\end{proof}

\begin{thm}
\label{thm:subopt}
For all $s\in OPEN\cup BE$, if $s$ is not $s_{start}$ or one of its optimal-path successors, then $bound(s) \le \epsilon g^*(s)$. Hence, for all $s\in CLOSED$, $g(s) \le v(s) \le \epsilon g^*(s)$.
\end{thm}

\begin{proof}
We proceed by induction on the order in which states are expanded.

Let $\pi = \langle s_0,s_1,\ldots,s_N \rangle$ be a minimum-cost path from $s_0 = s_{start}$ to $s_N = s\in OPEN\cup BE$. Choose the minimum $i$ such that $s_i\in OPEN\cup BE$.
There are two cases to consider, depending on whether $s_{i-1}\in CLOSED$.

If so, then $expand(s_{i-1})$ has assigned to $g_p(s_i)$. Hence by the induction hypothesis,
\begin{eqnarray*}
g_p(s_i) &\le& v(s_{i-1}) + \epsilon c(s_{i-1},s_i)
\\&\le& \epsilon g^*(s_{i-1}) + \epsilon c(s_{i-1},s_i)
\\&=& \epsilon g^*(s_i)
\end{eqnarray*}

On the other hand, suppose $s_{i-1}\notin CLOSED$. Choose the maximum $j<i$ such that $s_j\in CLOSED$, or $j=0$ if there is no such $j$. Then $j\le i-2$ and, by the induction hypothesis, $g(s_j)\le \epsilon g^*(s_j)$. Furthermore, $g(s_k) = v(s_k)$ for all $j<k<i$. Let $g_{old}(s_i)$ denote the value of $g(s_i)$ at the start of the current main() loop iteration. Then,
\begin{eqnarray*}
g_p(s_i) &\le& g_{old}(s_i) + 2(\epsilon-1)c_l
\\&\le& v(s_j) + c^*(s_j,s_i) + 2(\epsilon-1)c_l
\\&\le& \epsilon g^*(s_j) + c^*(s_j,s_i) + 2(\epsilon-1)c_l
\\&=& \epsilon (g^*(s_j) + c^*(s_j,s_i)) + (\epsilon-1)(2c_l - c^*(s_j,s_i))
\\&\le& \epsilon g^*(s_i)
\end{eqnarray*}

In both cases, we found that
\[g_p(s_i) + \epsilon h(s_i,s) \le \epsilon g^*(s_i) + \epsilon c^*(s_i,s) = \epsilon g^*(s).\]
Therefore, by Lemma \ref{lem:bound},
\[bound(s) \le \min_{s' \in OPEN \cup BE} g_p(s') + \epsilon h(s',s) \le \epsilon g^*(s).\]
\end{proof}

\begin{cor}
\label{cor:subopt}
At the end of a main() loop iteration, the path obtained by following the back-pointers $bp(\cdot)$ from $s_{goal}$ to $s_{start}$ is $\epsilon$-suboptimal.
\end{cor}

\begin{proof}
The termination condition of PARA* implies $g(s_{goal}) \le bound(s_{goal})$. By construction, the path given by following back-pointers costs at most $g(s_{goal})$. The claim now follows from Theorem \ref{thm:subopt}.
\end{proof}

\section{Performance Guarantees}

Consider a simplified version of PARA* which ignores the loop in bound($s$): we call it blind PARA*. In this case, no $g_p$ values need be computed nor stored, and $bound(s)$ is simply $g(s) + f_{min} - f(s) + (2\epsilon-w-1)c_l$ where $f_{min}$ is the minimum $f$-value in $OPEN \cup BE$. Blind PARA* can only expand states which would be proved safe with zero iterations of the bound($s$) loop in ordinary PARA*. Thus, all of the performance guarantees we prove for blind PARA* also hold for PARA*. 

\begin{thm}
\label{thm:depth}
If $w \le 1$, the parallel depth of blind PARA* is bounded above by
\[\min\left(\frac{\epsilon g^*(s_{goal})}{(1-w)c_l},\;
\frac{\left(\epsilon g^*(s_{goal})\right)^2 }{(4\epsilon-2w-2)c_l^2}\right).\]
\end{thm}

\begin{proof}
We prove the two bounds separately. For the first, note that if the lowest $f$-value is $f_{min}$, every state with $f$-value up to $f_{min} + (2\epsilon-w-1)c_l$ can simultaneously be expanded. Since $h$ is consistent, the successors' $f$-values is at least $f_{min} + (1-w)c_l$. Therefore, the depth is at most
\[\frac{\epsilon g^*(s_{goal})}{(1-w)c_l}\]

For the other bound, notice that since $f$-values never decrease along paths, once the minimum $f$-value in $OPEN$ surpasses $f_{min}$, from then on all nodes with $f$-value up to $f_{min} + (2\epsilon-w-1)c_l$ are always safe to expand. And during each iteration of the simultaneous expansions, the $g$-value of all such nodes increases by at least $c_l$. Since $g$ cannot exceed $f$, this continues for at most $(f_{min} + (2\epsilon-w-1)c_l) / c_l = f_{min}/c_l + 2\epsilon-w-1$ iterations, after which every node in $OPEN$ has $f$-value $\ge f_{min} + (2\epsilon-w-1)c_l$. Continuing this process until $f_{min}$ exceeds $\epsilon g^*(s_{goal})$, a bound on the total iteration count is:

$2\epsilon-w-1 + 2(2\epsilon-w-1) + 3(2\epsilon-w-1) + ... + \epsilon g^*(s_{goal})/c_l
\approxeq (\epsilon g^*(s_{goal})/c_l )^2 / ( 4\epsilon-2w-2 ).$
\end{proof}

\section{Edgewise Supobtimality}

Let $k(s)$ be the least number of edges used in a minimum-cost path to $s$ and fix $\delta > 0$. If $g_{front}$ and $g_{back}$ are each increased by $2\delta$, then by similar arguments to the proofs earlier in the paper, we find that, upon expanding $s$, $g(s) \le \epsilon g^*(s) + \delta k(s)$.

Here's an extension inspired by \cite{klein1997randomized}: suppose the mean edge cost $c_m$ along the optimal path is known to be much greater than the lower bound $c_l$. In such a case, the bound in Theorem \ref{thm:depth} scales poorly. To remedy the situation, we ``grow" the small edges, effectively running PARA* with $c_l' = c_l + \delta$ and $c'(s,s') = \max(c(s,s'), c_l')$.

\begin{thm}
\label{thm:delta}
If the mean cost of the edges along the minimum-cost path to $s$ is at least $c_m$, then upon expansion, $g(s) \le \epsilon(1+\delta/c_m)g^*(s)$. Therefore, to get the same optimality factor as $\epsilon$, we can set $\delta = (\epsilon-1)c_m$.
\end{thm}

\begin{proof}
We assumed $c_m \le g^*(s) / k(s)$, so $k(s) \le g^*(s) / c_m$.
It follows from Lemma \ref{thm:subopt} that $g'(s) \le \epsilon g'^*(s) \le \epsilon(g^*(s) + \delta k(s)) \le \epsilon(1+\delta/c_m)g^*(s)$.
\end{proof}

\begin{cor}
\label{cor:delta}
If $w \le 1$, the parallel depth of blind PARA* can be improved to
\[\frac{\epsilon g^*(s_{goal})}{(1-w)(c_l+(\epsilon-1)c_m)}.\]
\end{cor}

\bibliographystyle{aaai}
\bibliography{PARA}

\end{document}
